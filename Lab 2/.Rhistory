}
ROC
install.packages("kknn")
setwd("~/TDDE01/Machine-Learning-Labs/Lab 2")
install.packages("prcomp")
dataframe = read.csv("NIRSpectra.csv", dec=',')
dataframe = read.csv("NIRSpectra.csv")
dataframe = read.csv("NIRSpectra.csv")
dataframe = read.csv("NIRSpectra.csv", dec=',')
dataframe = read.csv("NIRSpectra.csv", dec=';')
dataframe = read.csv("NIRSpectra.csv", dec=',')
dataframe = read.csv("NIRSpectra.csv")
dataframe = read.csv2("NIRSpectra.csv")
res = prcomp(dataframe)
res
dataframe = read.csv2("NIRSpectra.csv")
dataframe
View(dataframe)
dataframe$Viscosity
data = dataframe
data$Viscosity = c()
res = prcomp(data)
λ = res$sdev^2
λ = res$sdev^2
λ
res$sdev
help("prcomp")
sprint("%2/3f", λ/sum(λ)*100)
library(screeplot)
install.packages("screeplot")
library(biplot)
sprintf("%2/3f", λ/sum(λ)*100)
sprintf("%2.3f", λ/sum(λ)*100)
var_props = λ/sum(λ)*100
sprintf("%2.3f", var_props)
hist(var_props)
hist(var_props, breaks=99)
hist(var_props, breaks=100)
hist(sort(var_props), breaks=100)
var_props = λ/sum(λ)*100
var_probs
var_props
sprintf("%2.3f", var_props)
var_probs[1:2]
var_props[1:2]
sum(var_props[1:2])
hist(var_props[1:10])
hist(seq(1:10), var_props[1:10])
help(plot)
plot(seq(1:10), var_props[1:10])
plot(seq(1:10), var_props[1:10], type="h")
plot(seq(1:10), var_props[1:10], type="s")
plot(seq(1:10), var_props[1:10], type="S")
plot(seq(1:10), var_props[1:10], type="l")
help(hist)
hist(var_props[1:10], freq=FALSE)
var_props = λ/sum(λ)*100
sprintf("%2.3f", var_props)
hist(var_props[1:10], freq=FALSE)
hist(var_props[1:10], ylim=c(0,1), freq=FALSE)
hist(var_props[1:10], ylim=c(0,1))
hist(var_props[1:10], prob=TRUE
hist(var_props, prob=TRUE
hist(var_props, prob=TRUE)
hist(var_props[1:10], prob=TRUE)
plot(var_props[1:10], prob=TRUE)
barplot(var_props[1:10], prob=TRUE)
barplot(var_props[1:100], prob=TRUE)
0
barplot(var_props[1:10], prob=TRUE)
barplot(var_props[1:10],)
barplot(var_props[1:10])
,
, prob=TRUE
barplot(var_props[1:10], xlim(0,10))
barplot(var_props[1:10], ylim=c(0,1))
barplot(var_props[1:10], ylim=c(0,100))
barplot(var_props[1:10], ylim=c(0,100), col="forestgreen")
barplot(var_props[1:10], ylim=c(0,100), col="forestgreen", main="Variation proportions for different eigenvalues")
barplot(var_props[1:10], ylim=c(0,100), col="forestgreen", main="Variation proportions for different eigenvalues", xlab="λi", ylab="Varaible proportion")
barplot(var_props[1:10], ylim=c(0,100), col="forestgreen",
main="Variation proportions for different eigenvalues", xlab="λi", ylab="Varaible proportion")
legend("topright", col=c("forestgreen"), legend=c("Pc"))
sum(var_props[1:2]) # PC1 and PC2 count for 99.5957% of the variation
plot(fit_dt_dev)
text(fit_dt_dev, pretty=0)
dataframe = read.csv("creditscoring.csv", dec=',')
n = dim(dataframe)[1]
set.seed(12345)
ids = sample(1:n, n) # sample random order of data
training = dataframe[ids[1:floor(n/2)],] # 50% of the data
validation = dataframe[ids[(floor(n/2)+1):floor(3*n/4)],] # 25% of data
test = dataframe[ids[(floor(3*n/4)+1):n],] # the last 25% of the data
fit_dt_dev = tree(good_bad~., data=training, split="deviance")
fit_dt_gini = tree(good_bad~., data=training, split="gini")
plot(fit_dt_dev)
text(fit_dt_dev, pretty=0)
dataframe = read.csv("creditscoring.csv", dec=',')
n = dim(dataframe)[1]
set.seed(12345)
ids = sample(1:n, n) # sample random order of data
training = dataframe[ids[1:floor(n/2)],] # 50% of the data
validation = dataframe[ids[(floor(n/2)+1):floor(3*n/4)],] # 25% of data
test = dataframe[ids[(floor(3*n/4)+1):n],] # the last 25% of the data
fit_dt_dev = tree(good_bad~., data=training, split="deviance")
library(tree)
library(e1071)
fit_dt_dev = tree(good_bad~., data=training, split="deviance")
fit_dt_gini = tree(good_bad~., data=training, split="gini")
plot(fit_dt_dev)
text(fit_dt_dev, pretty=0)
plot(fit_dt_gini)
text(fit_dt_gini, pretty=0)
plot(fit_dt_dev)
text(fit_dt_dev, pretty=0)
dataframe = read.csv2("NIRSpectra.csv")
data = dataframe
data$Viscosity = c()
res = prcomp(data)
λ = res$sdev^2
var_props = λ/sum(λ)*100
sprintf("%2.3f", var_props)
barplot(var_props[1:10], ylim=c(0,100), col="forestgreen",
main="Variation proportions for different eigenvalues",
xlab="λi", ylab="Varaible proportion")
sum(var_props[1:2]) # PC1 and PC2 count for 99.5957% of the variation
U = res$rotation
head(U)
plot(res$x[,1], res$[x,2])
plot(res$x[,1], res$x[,2])
plot(res$x[,1], res$x[,2], xlim=c(-1,2), ylim=c(-1,2))
plot(res$x[,1], res$x[,2], xlim=c(0,2), ylim=c(0,2))
plot(res$x[,1], res$x[,2], xlim=c(-1,2), ylim=c(-1,2))
plot(res$x[,1], res$x[,2], xlim=c(-1,2), ylim=c(-0.5,0.5))
plot(res$x[,1], res$x[,2])
plot(res$x[,1], res$x[,2]) #
plot(res$x[,1], res$x[,2]) # 2 "strong" outliers, 5-7 "medium" outliers
plot(U[,1], main="Traceplot for PC1")
plot(U[,2], main="Traceplot for PC2")
plot(U[,2], main="Traceplot for PC2")
plot(U[,1], main="Traceplot for PC1")
plot(U[,2], main="Traceplot for PC2")
plot(U[,1], main="Traceplot for PC1")
plot(U[,1], main="Traceplot for PC1")
plot(U[,1], main="Traceplot for PC1", ylim=c(0,0.11))
plot(U[,2], main="Traceplot for PC2") # the last few original feutures mainly explain this PC
plot(U[,2], main="Traceplot for PC2", ylim=c(-0.1, 0.3)) # the last few original feutures mainly explain this PC
plot(U[,2], main="Traceplot for PC2", ylim=c(-0.3, 0.3)) # the last few original feutures mainly explain this PC
plot(U[,1], main="Traceplot for PC1", ylim=c(-0.11,0.11))
plot(U[,2], main="Traceplot for PC2", ylim=c(-0.3, 0.3)) # the last few original feutures mainly explain this PC
library(tree)
library(e1071)
library(MASS)
# Task 1
dataframe = read.csv("creditscoring.csv", dec=',')
n = dim(dataframe)[1]
set.seed(12345)
ids = sample(1:n, n) # sample random order of data
# Split the data into training/validation/training (50/25/25)
training = dataframe[ids[1:floor(n/2)],] # 50% of the data
validation = dataframe[ids[(floor(n/2)+1):floor(3*n/4)],] # 25% of data
test = dataframe[ids[(floor(3*n/4)+1):n],] # the last 25% of the data
# Task 2
fit_dt_dev = tree(good_bad~., data=training, split="deviance")
fit_dt_gini = tree(good_bad~., data=training, split="gini")
# plot(fit_dt_dev)
# text(fit_dt_dev, pretty=0)
# plot(fit_dt_gini)
# text(fit_dt_gini, pretty=0)
# Fit Test Data
fitted_test_dev = predict(fit_dt_dev, test, type="class")
fitted_test_gini = predict(fit_dt_gini, test, type="class")
# Confusion matrixes
cm_dev_test= table(fitted_test_dev, test[,20])
cm_gini_test = table(fitted_test_gini, test[,20])
# Missclassification rates
mcr_dev_test = 1-sum(diag(cm_dev_test))/sum(cm_dev_test)
mcr_gini_test = 1-sum(diag(cm_gini_test))/sum(cm_gini_test)
#mcr_dev_training = 0.2105 = 104 / 494
W`= data%*%U
W`
W= data%*%U
U
W= data%*%t(U)
help(prcomp)
W= data%*%res$rotation
W= data%*%t(res$rotation)
data
res$rotation
dim(data)
dim(res$rotation)
dim(U)
U
W= res$x%*%res$rotation
W
plot(W[,1], main="Traceplot for PC1", ylim=c(-0.11,0.11))
dim(res$x)
W= dataframe%*%res$rotation
W= res$x%*%res$rotation
plot(W[,1], main="Traceplot for PC1", ylim=c(-0.11,0.11))
dim(dataframe)
dim(data)
W= data%*%res$rotation
res$x
W= res$x%*%res$rotation
W
W`= res$x%*%res$rotation
Wtick= res$x%*%res$rotation
plot(Wtick[,1], main="Traceplot for PC1", ylim=c(-0.11,0.11))
plot(Wtick[,1], main="Traceplot for PC1", ylim=c(-0.11,0.11))
plot(Wtick[,2], main="Traceplot for PC2", ylim=c(-0.3, 0.3))
cm_dev_test= table(fitted_test_dev, test[,20])
cm_dev_test
library(tree)
library(e1071)
library(MASS)
# Task 1
dataframe = read.csv("creditscoring.csv", dec=',')
n = dim(dataframe)[1]
set.seed(12345)
ids = sample(1:n, n) # sample random order of data
# Split the data into training/validation/training (50/25/25)
training = dataframe[ids[1:floor(n/2)],] # 50% of the data
validation = dataframe[ids[(floor(n/2)+1):floor(3*n/4)],] # 25% of data
test = dataframe[ids[(floor(3*n/4)+1):n],] # the last 25% of the data
# Task 2
fit_dt_dev = tree(good_bad~., data=training, split="deviance")
fit_dt_gini = tree(good_bad~., data=training, split="gini")
# plot(fit_dt_dev)
# text(fit_dt_dev, pretty=0)
# plot(fit_dt_gini)
# text(fit_dt_gini, pretty=0)
# Fit Test Data
fitted_test_dev = predict(fit_dt_dev, test, type="class")
fitted_test_gini = predict(fit_dt_gini, test, type="class")
# Confusion matrixes
cm_dev_test= table(fitted_test_dev, test[,20])
cm_gini_test = table(fitted_test_gini, test[,20])
# Missclassification rates
mcr_dev_test = 1-sum(diag(cm_dev_test))/sum(cm_dev_test)
mcr_gini_test = 1-sum(diag(cm_gini_test))/sum(cm_gini_test)
#mcr_dev_training = 0.2105 = 104 / 494
#mcr_gini_training = 0.2368 = 117 / 494
# mcr_dev < mcr_gini --> Will use Deviance as the measure of impurity
# Task 3
# Empty vectors of scores
train_score = rep(0,14)
validation_score = rep(0,14)
# Compute the deviance for the pruned tree's prediction on train set and on validation set, save in score vectors
for(i in 2:14){
pruned_tree = prune.tree(fit_dt_dev, best=i)
prediction = predict(pruned_tree, newdata=validation, type="tree")
train_score[i] = deviance(pruned_tree)
validation_score[i] = deviance(prediction)
}
# Plot the deviances for each score
plot(2:14, train_score[2:14], type="b", col="green", xlab="# of leaves", ylab="deviance", ylim=c(250, 600))
points(2:14, validation_score[2:14], type="b", col="blue")
legend("topright", legend=c("Train Score", "Validation Score"), col=c("green","blue"))
# Optimal leaves = 4, depth=4, variables=savings, duration, history
pruned_tree = prune.tree(fit_dt_dev, best=4)
plot(pruned_tree)
text(pruned_tree, pretty=0)
# Predictions for pruned tree on TEST data
fitted_pruned_tree_test = predict(pruned_tree, newdata=test, type="class")
# Confusion matrix for pruned tree on the TEST data
cm_dev_test_prun = table(fitted_pruned_tree_test, test[,20])
# Missclassification rate for pruned tree on the TEST data
mcr_dev_test_prun = 1-sum(diag(cm_dev_test_prun)/sum(cm_dev_test_prun)) #0.248
# Task 4
fit_naive_bayes =naiveBayes(good_bad~., data=training)
fitted_training_naive_bayes = predict(fit_naive_bayes, newdata=training, type="class")
fitted_test_naive_bayes = predict(fit_naive_bayes, newdata=test, type="class")
# Confusion matrixes
cm_naive_bayes_training = table(fitted_training_naive_bayes, training[,20])
cm_naive_bayes_test = table(fitted_test_naive_bayes, test[,20])
# Missclassification rates
mcr_naive_bayes_training = 1-sum(diag(cm_naive_bayes_training))/sum(cm_naive_bayes_training) # 0.3
mcr_naive_bayes_test = 1-sum(diag(cm_naive_bayes_test))/sum(cm_naive_bayes_test) # 0.3
# Task 5  - FEL
fit_naive_bayes_loss = naiveBayes(good_bad~., data=training)
fitted_training_naive_bayes_loss = predict(fit_naive_bayes_loss, newdata=training, type="class")
fitted_test_naive_bayes_loss = predict(fit_naive_bayes_loss, newdata=test, type="class")
# Confusion matrixes
cm_naive_bayes_training_loss  = table(fitted_training_naive_bayes_loss , training[,20])
cm_naive_bayes_test_loss  = table(fitted_test_naive_bayes_loss , test[,20])
# Missclassification rates
mcr_naive_bayes_training_loss  = 1-sum(diag(cm_naive_bayes_training_loss ))/sum(cm_naive_bayes_training_loss ) # 0.3
mcr_naive_bayes_test_loss  = 1-sum(diag(cm_naive_bayes_test_loss ))/sum(cm_naive_bayes_test_loss ) # 0.3
library(tree)
library(e1071)
library(MASS)
# Task 1
install.packages("e1071")
library(tree)
library(e1071)
library(MASS)
dataframe = read.csv("creditscoring.csv", dec=',')
n = dim(dataframe)[1]
set.seed(12345)
ids = sample(1:n, n) # sample random order of data
training = dataframe[ids[1:floor(n/2)],] # 50% of the data
validation = dataframe[ids[(floor(n/2)+1):floor(3*n/4)],] # 25% of data
test = dataframe[ids[(floor(3*n/4)+1):n],] # the last 25% of the data
fit_dt_dev = tree(good_bad~., data=training, split="deviance")
fit_dt_gini = tree(good_bad~., data=training, split="gini")
fitted_test_dev = predict(fit_dt_dev, test, type="class")
fitted_test_gini = predict(fit_dt_gini, test, type="class")
# Confusion matrixes
cm_dev_test= table(fitted_test_dev, test[,20])
cm_gini_test = table(fitted_test_gini, test[,20])
# Missclassification rates
mcr_dev_test = 1-sum(diag(cm_dev_test))/sum(cm_dev_test)
mcr_gini_test = 1-sum(diag(cm_gini_test))/sum(cm_gini_test)
#mcr_dev_training = 0.2105 = 104 / 494
#mcr_gini_training = 0.2368 = 117 / 494
# mcr_dev < mcr_gini --> Will use Deviance as the measure of impurity
# Task 3
# Empty vectors of scores
train_score = rep(0,14)
validation_score = rep(0,14)
# Compute the deviance for the pruned tree's prediction on train set and on validation set, save in score vectors
for(i in 2:14){
pruned_tree = prune.tree(fit_dt_dev, best=i)
prediction = predict(pruned_tree, newdata=validation, type="tree")
train_score[i] = deviance(pruned_tree)
validation_score[i] = deviance(prediction)
}
# Plot the deviances for each score
plot(2:14, train_score[2:14], type="b", col="green", xlab="# of leaves", ylab="deviance", ylim=c(250, 600))
points(2:14, validation_score[2:14], type="b", col="blue")
legend("topright", legend=c("Train Score", "Validation Score"), col=c("green","blue"))
legend("topright", legend=c("(green) Train Score", "(blue) Validation Score"), col=c("green","blue"))
plot(2:14, train_score[2:14], type="b", col="green", xlab="# of leaves", ylab="deviance", ylim=c(250, 600))
points(2:14, validation_score[2:14], type="b", col="blue")
legend("topright", legend=c("(green) Train Score", "(blue) Validation Score"), col=c("green","blue"))
plot(U[,1], main="Traceplot for PC1", ylim=c(-0.11,0.11))
plot(U[,2], main="Traceplot for PC2", ylim=c(-0.3, 0.3)) # the last few original feutures mainly explain this PC
Wtick= res$x%*%res$rotation
library(biplot)
dataframe = read.csv2("NIRSpectra.csv")
data = dataframe
data$Viscosity = c()
res = prcomp(data)
# Eigenvalues
λ = res$sdev^2
dataframe = read.csv2("NIRSpectra.csv")
data = dataframe
data$Viscosity = c()
res = prcomp(data)
λ = res$sdev^2
var_props = λ/sum(λ)*100
sprintf("%2.3f", var_props)
barplot(var_props[1:10], ylim=c(0,100), col="forestgreen",
main="Variation proportions for different eigenvalues",
xlab="λi", ylab="Varaible proportion") # The plot shows that 2 PCs should be extracted
sum(var_props[1:2]) # PC1 and PC2 count for 99.5957% of the variation
plot(res$x[,1], res$x[,2]) # 2 "strong" outliers, 5-7 "medium" outliers
U = loadings(res) # loadings
plot(U[,1], main="Traceplot for PC1", ylim=c(-0.11,0.11))
plot(U[,2], main="Traceplot for PC2", ylim=c(-0.3, 0.3)) # the last few original feutures mainly explain this PC
var_props = λ/sum(λ)*100
barplot(var_props[1:10], ylim=c(0,100), col="forestgreen",
barplot(var_props[1:10], ylim=c(0,100), col="forestgreen",
main="Variation proportions for different eigenvalues",
xlab="λi", ylab="Varaible proportion") # The plot shows that 2 PCs should be extracted
sum(var_props[1:2]) # PC1 and PC2 count for 99.5957% of the variation
barplot(var_props[1:10], ylim=c(0,100), col="forestgreen",
main="Variation proportions for different eigenvalues",
xlab="λi", ylab="Varaible proportion") # The plot shows that 2 PCs should be extracted
sum(var_props[1:2]) # PC1 and PC2 count for 99.5957% of the variation
U = loadings(res) # loadings
plot(U[,1], main="Traceplot for PC1", ylim=c(-0.11,0.11))
plot(U[,2], main="Traceplot for PC2", ylim=c(-0.3, 0.3)) # the last few original feutures mainly explain this PC
dataframe = read.csv2("NIRSpectra.csv")
data = dataframe
data$Viscosity = c()
res = prcomp(data)
λ = res$sdev^2
var_props = λ/sum(λ)*100
barplot(var_props[1:10], ylim=c(0,100), col="forestgreen",
main="Variation proportions for different eigenvalues",
barplot(var_props[1:10], ylim=c(0,100), col="forestgreen",
main="Variation proportions for different eigenvalues",
xlab="λi", ylab="Varaible proportion") # The plot shows that 2 PCs should be extracted
sum(var_props[1:2]) # PC1 and PC2 count for 99.5957% of the variation
barplot(var_props[1:10], ylim=c(0,100), col="forestgreen",
main="Variation proportions for different eigenvalues",
xlab="λi", ylab="Varaible proportion") # The plot shows that 2 PCs should be extracted
sum(var_props[1:2]) # PC1 and PC2 count for 99.5957% of the variation
plot(res$x[,1], res$x[,2]) # 2 "strong" outliers, 5-7 "medium" outliers
U = loadings(res) # loadings
plot(U[,1], main="Traceplot for PC1", ylim=c(-0.11,0.11))
plot(U[,1], main="Traceplot for PC1", xlim=c(0, 20), ylim=c(-0.11,0.11))
plot(U[,1], main="Traceplot for PC1", xlim=c(-1,1), ylim=c(-0.11,0.11))
U
res = prcomp(data)
res
U = loadings(res) # loadings
U
dataframe = read.csv2("NIRSpectra.csv")
dataframe
data = dataframe
data$Viscosity = c()
res = prcomp(data)
res
loadings(res)
plot(U[,1], main="Traceplot for PC1", ylim=c(-0.11,0.11))
res = prcomp(data)
res
λ = res$sdev^2
loadings(res)

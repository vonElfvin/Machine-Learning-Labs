}
ROC
install.packages("kknn")
setwd("~/TDDE01/Machine-Learning-Labs/Lab 2")
data
library(fastICA)
library(pls)
dataframe = read.csv2("NIRSpectra.csv")
data = dataframe
data$Viscosity = c()
res = prcomp(data)
# Eigenvalues
位 = res$sdev^2
# Variation proportions for the different eigenvalues
var_props = 位/sum(位)*100
sprintf("%2.3f", var_props)
barplot(var_props[1:10], ylim=c(0,100), col="forestgreen",
main="Variation proportions for different eigenvalues",
xlab="位i", ylab="Varaible proportion") # The plot shows that 2 PCs should be extracted
sum(var_props[1:2]) # PC1 and PC2 count for 99.5957% of the variation
# Scores - There are at least 2 unusual diesiel fuels according to this plot
plot(res$x[,1], res$x[,2]) # 2 "strong" outliers, 5-7 "medium" outliers
# Task 2
U = res$rotation
# Tracing plots
plot(U[,1], main="Traceplot for PC1", ylim=c(-0.11,0.11))
plot(U[,2], main="Traceplot for PC2", ylim=c(-0.3, 0.3)) # the last few original feutures mainly explain this PC
# Task 3 - ICA
ica = fastICA(data, 2, alg.typ="parallel", fun="logcosh", alpha=1, method="R", row.norm=FALSE, maxit=200, tol=0.0001, verbose=TRUE)
Wtick = ica$K%*%ica$W
plot(Wtick[,1], main="Traceplot, W' column 1")
plot(Wtick[,2], main="Traceplot, W' column 2")
# Scores
plot(ica$S[,1], ica$S[,2]) # ICA,
# Task 4
set.seed(12345)
# PCr lec 2.d
data
dataframe
set.seed(12345)
pcr = pcr(Viscosity~., data=dataframe, validation="CV")
pcr
summary(pcr)
validationplot(pcr, val.type="MSEP")
pcr = pcr(Viscosity~., data=dataframe, validation="CV")
validationplot(pcr, val.type="MSEP")
validationplot(pcr, val.type="MSEP", main="Dependence of the MSEP and #components")
dataframe = csv2.read("state.csv")
dataframe = read.cs2("state.csv")
dataframe = read.csv2("state.csv")
dataframe = read.csv("State.csv", dec=',')
dataframe = read.csv2("State.csv", dec=',')
dataframe = read.csv2("State.csv")
dataframe
help(sort)
sort(dataframe, partial="MET")
sorted = dataframe[order(MET)]
sorted = dataframe[order("MET")]
sorted
sorted = dataframe[order("MET"),]
sorted
sorted = dataframe[,order("MET")]
sorted
sorted = dataframe[order("MET"),]
sorted
dataframe
sorted = dataframe[order(dataframe["MET"]),]
sorted
plot(sorted$EX, sorted$MET)
data = read.csv2("State.csv")
data.sorted = data[order(dataframe["MET"]),]
plot(data.sorted$EX, data.sorted$MET)
data.sorted = data[order(dataframe["MET"]),]
data = read.csv2("State.csv")
data.sorted = data[order(data["MET"]),]
plot(data.sorted$EX, data.sorted$MET)
plot(data$EX, data$MET)
barplot(data.sorted$EX, data.sorted$MET)
hist(data.sorted$EX, data.sorted$MET)
plot
hist(data.sorted$EX, data.sorted$MET)
barplot(data.sorted$EX, data.sorted$MET)
hist(data.sorted$EX, data.sorted$MET)
plot(data.sorted$EX, data.sorted$MET)
plot(data.sorted$EX, data.sorted$MET, type="l")
plot(data.sorted$EX, data.sorted$MET)
plot(data.sorted$EX, data.sorted$MET, ylim=c(0,100))
plot(data.sorted$EX, data.sorted$MET, xlim=c(0,450), ylim=c(0,100))
plot(data.sorted$MET, data.sorted$EX, xlim=c(0,450), ylim=c(0,100))
plot(data.sorted$MET, data.sorted$EX, ylim=c(0,450), xlim=c(0,100))
library(tree)
help(tree.control)
tree.control(minsize=8)
help("tree")
tree.regression = cv.tree(EX~MET, data=data.sorted)
help(tree.control)
length(data.sorted)
dim(data.sorted)
nobs = dim(data.sorted)[1]
tree.regression = cv.tree(EX~MET, data=data.sorted).control(nobs, minsize=8)
tree.control(nobs, minsize=8)
tree.regression = cv.tree(EX~MET, data=data.sorted)
tree.regression = cv.tree(formula="EX~MET", data=data.sorted)
help(tree)
tree.regression = cv.tree(formula="EX~MET", data=data.sorted, control=control)
help(cv.tree)
tree.regression = tree(EX~MET., data=data.sorted, split="regression")
help(tree)
tree.regression = tree(EX~MET., data=data.sorted)
tree.regression = tree(formula="EX~MET", data=data.sorted)
tree.regression = tree(EX~MET, data=data.sorted)
nobs = dim(data.sorted)[1]
control = tree.control(nobs, minsize=8)
help(cv.tree)
help(tree)
tree.control = tree.control(nobs, minsize=8)
tree.regression = tree(EX~MET, data=data.sorted, control=tree.control)
tree.regression
help(cv.tree)
tree = cv.tree(tree.regression)
tree.regression.cv = cv.tree(tree.regression)
tree.regression.cv
library(tree)
data = read.csv2("State.csv")
data.sorted = data[order(data["MET"]),]
plot(data.sorted$MET, data.sorted$EX, ylim=c(0,450), xlim=c(0,100)) # Some sort of regressional model, 2nd polynomial
nobs = dim(data.sorted)[1]
tree.control = tree.control(nobs, minsize=8)
tree.regression = tree(EX~MET, data=data.sorted, control=tree.control)
tree.regression.cv = cv.tree(tree.regression)
tree.regression
summary(tree.regression)
tree.regression$weights
summary(tree.regression.cv)
tree.regression.cv$k
tree.regression.cv$size
tree.regression.cv$k
tree.regression.cv$method
tree.regression.cv$dev
tree.regression.cv$k
tree.regression.cv$size
fit.tree.regression = tree(EX~MET, data=data.sorted, control=tree.control)
fit.tree.regression = tree(EX~MET, data=data.sorted, control=tree.control)
set.seed(12354)
cv.tree.regression = cv.tree(tree.regression)
plot(cv.tree.regression$size, cv.tree.regression$dev, type="b", col="forestgreen")
plot(cv.tree.regression$k, cv.tree.regression$dev, type="b", col="forestgreen")
plot(cv.tree.regression$size, cv.tree.regression$dev, type="b", col="forestgreen")
plot(log(cv.tree.regression$k), cv.tree.regression$dev, type="b", col="forestgreen")
library(tree)
library(e1071)
library(MASS)
# Task 1
dataframe = read.csv("creditscoring.csv", dec=',')
n = dim(dataframe)[1]
set.seed(12345)
ids = sample(1:n, n) # sample random order of data
# Split the data into training/validation/training (50/25/25)
training = dataframe[ids[1:floor(n/2)],] # 50% of the data
validation = dataframe[ids[(floor(n/2)+1):floor(3*n/4)],] # 25% of data
test = dataframe[ids[(floor(3*n/4)+1):n],] # 25% of the data
# Task 2
fit_dt_dev = tree(good_bad~., data=training, split="deviance")
fit_dt_gini = tree(good_bad~., data=training, split="gini")
# plot(fit_dt_dev)
# text(fit_dt_dev, pretty=0)
# plot(fit_dt_gini)
# text(fit_dt_gini, pretty=0)
# Fit Test Data
fitted_test_dev = predict(fit_dt_dev, test, type="class")
fitted_test_gini = predict(fit_dt_gini, test, type="class")
# Confusion matrixes
cm_dev_test= table(fitted_test_dev, test[,20])
cm_gini_test = table(fitted_test_gini, test[,20])
# Missclassification rates
mcr_dev_test = 1-sum(diag(cm_dev_test))/sum(cm_dev_test)
mcr_gini_test = 1-sum(diag(cm_gini_test))/sum(cm_gini_test)
#mcr_dev_training = 0.2105 = 104 / 494
#mcr_gini_training = 0.2368 = 117 / 494
# mcr_dev < mcr_gini --> Will use Deviance as the measure of impurity
# Task 3
# Empty vectors of scores
train_score = rep(0,14)
validation_score = rep(0,14)
# Compute the deviance for the pruned tree's prediction on train set and on validation set, save in score vectors
for(i in 2:14){
pruned_tree = prune.tree(fit_dt_dev, best=i)
prediction = predict(pruned_tree, newdata=validation, type="tree")
train_score[i] = deviance(pruned_tree)
validation_score[i] = deviance(prediction)
}
# Plot the deviances for each score
plot(2:14, train_score[2:14], type="b", col="green", xlab="# of leaves", ylab="deviance", ylim=c(250, 600))
points(2:14, validation_score[2:14], type="b", col="blue")
legend("topright", legend=c("(green) Train Score", "(blue) Validation Score"), col=c("green","blue"))
# Optimal leaves = 4, depth=4, variables=savings, duration, history
pruned_tree = prune.tree(fit_dt_dev, best=4)
plot(pruned_tree)
text(pruned_tree, pretty=0)
# Predictions for pruned tree on TEST data
fitted_pruned_tree_test = predict(pruned_tree, newdata=test, type="class")
# Confusion matrix for pruned tree on the TEST data
cm_dev_test_prun = table(fitted_pruned_tree_test, test[,20])
# Missclassification rate for pruned tree on the TEST data
mcr_dev_test_prun = 1-sum(diag(cm_dev_test_prun)/sum(cm_dev_test_prun)) #0.248
# Task 4 - Naive Bayes
fit_naive_bayes =naiveBayes(good_bad~., data=training)
fitted_training_naive_bayes = predict(fit_naive_bayes, newdata=training, type="class")
fitted_test_naive_bayes = predict(fit_naive_bayes, newdata=test, type="class")
# Confusion matrixes
cm_naive_bayes_training = table(fitted_training_naive_bayes, training[,20])
cm_naive_bayes_test = table(fitted_test_naive_bayes, test[,20])
# Missclassification rates
mcr_naive_bayes_training = 1-sum(diag(cm_naive_bayes_training))/sum(cm_naive_bayes_training) # 0.3
mcr_naive_bayes_test = 1-sum(diag(cm_naive_bayes_test))/sum(cm_naive_bayes_test) # 0.3
# Task 5 - Loss Matrix
fitted_training_naive_bayes_prob = predict(fit_naive_bayes, newdata=training, type="raw")
fitted_test_naive_bayes_prob = predict(fit_naive_bayes, newdata=test, type="raw")
# Confusion matrixes
cm_naive_bayes_training_loss_matrix = table(fitted_training_naive_bayes_prob[,1]>10/11, training[,20])
cm_naive_bayes_test_loss_matrix = table(fitted_test_naive_bayes_prob[,1]>10/11, test[,20])
# Empty vectors of scores
train_score = rep(0,14)
validation_score = rep(0,14)
# Compute the deviance for the pruned tree's prediction on train set and on validation set, save in score vectors
for(i in 2:14){
pruned_tree = prune.tree(fit_dt_dev, best=i)
prediction = predict(pruned_tree, newdata=validation, type="tree")
train_score[i] = deviance(pruned_tree)
validation_score[i] = deviance(prediction)
}
# Plot the deviances for each score
plot(2:14, train_score[2:14], type="b", col="green", xlab="# of leaves", ylab="deviance", ylim=c(250, 600))
points(2:14, validation_score[2:14], type="b", col="blue")
legend("topright", legend=c("(green) Train Score", "(blue) Validation Score"), col=c("green","blue"))
plot(log(cv.tree.regression$k), cv.tree.regression$dev, type="b", col="forestgreen")
library(tree)
data = read.csv2("State.csv")
data.sorted = data[order(data["MET"]),]
plot(data.sorted$MET, data.sorted$EX, ylim=c(0,450), xlim=c(0,100)) # Some sort of regressional model, 2nd polynomial
nobs = dim(data.sorted)[1]
tree.control = tree.control(nobs, minsize=8)
fit.tree.regression = tree(EX~MET, data=data.sorted, control=tree.control)
set.seed(12354)
cv.tree.regression = cv.tree(tree.regression)
plot(cv.tree.regression$size, cv.tree.regression$dev, type="b", col="forestgreen")
cv.tree.regression = cv.tree(tree.regression)
cv.tree.regression = cv.tree(fit.tree.regression)
plot(cv.tree.regression$size, cv.tree.regression$dev, type="b", col="forestgreen")
plot(log(cv.tree.regression$k), cv.tree.regression$dev, type="b", col="forestgreen")
cv.tree.regression = cv.tree(fit.tree.regression)
cv.tree.regression
plot(cv.tree.regression$size, cv.tree.regression$dev, type="b", col="forestgreen")
plot(log(cv.tree.regression$k), cv.tree.regression$dev, type="b", col="forestgreen")
library(tree)
library(e1071)
library(MASS)
# Task 1
dataframe = read.csv("creditscoring.csv", dec=',')
n = dim(dataframe)[1]
set.seed(12345)
ids = sample(1:n, n) # sample random order of data
# Split the data into training/validation/training (50/25/25)
training = dataframe[ids[1:floor(n/2)],] # 50% of the data
validation = dataframe[ids[(floor(n/2)+1):floor(3*n/4)],] # 25% of data
test = dataframe[ids[(floor(3*n/4)+1):n],] # 25% of the data
# Task 2
fit_dt_dev = tree(good_bad~., data=training, split="deviance")
fit_dt_gini = tree(good_bad~., data=training, split="gini")
# plot(fit_dt_dev)
# text(fit_dt_dev, pretty=0)
# plot(fit_dt_gini)
# text(fit_dt_gini, pretty=0)
# Fit Test Data
fitted_test_dev = predict(fit_dt_dev, test, type="class")
fitted_test_gini = predict(fit_dt_gini, test, type="class")
# Confusion matrixes
cm_dev_test= table(fitted_test_dev, test[,20])
cm_gini_test = table(fitted_test_gini, test[,20])
# Missclassification rates
mcr_dev_test = 1-sum(diag(cm_dev_test))/sum(cm_dev_test)
mcr_gini_test = 1-sum(diag(cm_gini_test))/sum(cm_gini_test)
#mcr_dev_training = 0.2105 = 104 / 494
#mcr_gini_training = 0.2368 = 117 / 494
# mcr_dev < mcr_gini --> Will use Deviance as the measure of impurity
# Task 3
# Empty vectors of scores
train_score = rep(0,14)
validation_score = rep(0,14)
# Compute the deviance for the pruned tree's prediction on train set and on validation set, save in score vectors
for(i in 2:14){
pruned_tree = prune.tree(fit_dt_dev, best=i)
prediction = predict(pruned_tree, newdata=validation, type="tree")
train_score[i] = deviance(pruned_tree)
validation_score[i] = deviance(prediction)
}
# Plot the deviances for each score
plot(2:14, train_score[2:14], type="b", col="green", xlab="# of leaves", ylab="deviance", ylim=c(250, 600))
points(2:14, validation_score[2:14], type="b", col="blue")
legend("topright", legend=c("(green) Train Score", "(blue) Validation Score"), col=c("green","blue"))
# Optimal leaves = 4, depth=4, variables=savings, duration, history
pruned_tree = prune.tree(fit_dt_dev, best=4)
plot(pruned_tree)
text(pruned_tree, pretty=0)
# Predictions for pruned tree on TEST data
fitted_pruned_tree_test = predict(pruned_tree, newdata=test, type="class")
# Confusion matrix for pruned tree on the TEST data
cm_dev_test_prun = table(fitted_pruned_tree_test, test[,20])
# Missclassification rate for pruned tree on the TEST data
mcr_dev_test_prun = 1-sum(diag(cm_dev_test_prun)/sum(cm_dev_test_prun)) #0.248
# Task 4 - Naive Bayes
fit_naive_bayes =naiveBayes(good_bad~., data=training)
fitted_training_naive_bayes = predict(fit_naive_bayes, newdata=training, type="class")
fitted_test_naive_bayes = predict(fit_naive_bayes, newdata=test, type="class")
# Confusion matrixes
cm_naive_bayes_training = table(fitted_training_naive_bayes, training[,20])
cm_naive_bayes_test = table(fitted_test_naive_bayes, test[,20])
# Missclassification rates
mcr_naive_bayes_training = 1-sum(diag(cm_naive_bayes_training))/sum(cm_naive_bayes_training) # 0.3
mcr_naive_bayes_test = 1-sum(diag(cm_naive_bayes_test))/sum(cm_naive_bayes_test) # 0.3
# Task 5 - Loss Matrix
fitted_training_naive_bayes_prob = predict(fit_naive_bayes, newdata=training, type="raw")
fitted_test_naive_bayes_prob = predict(fit_naive_bayes, newdata=test, type="raw")
# Confusion matrixes
cm_naive_bayes_training_loss_matrix = table(fitted_training_naive_bayes_prob[,1]>10/11, training[,20])
cm_naive_bayes_test_loss_matrix = table(fitted_test_naive_bayes_prob[,1]>10/11, test[,20])
rain_score = rep(0,14)
validation_score = rep(0,14)
# Compute the deviance for the pruned tree's prediction on train set and on validation set, save in score vectors
for(i in 2:14){
pruned_tree = prune.tree(fit_dt_dev, best=i)
prediction = predict(pruned_tree, newdata=validation, type="tree")
train_score[i] = deviance(pruned_tree)
validation_score[i] = deviance(prediction)
}
# Plot the deviances for each score
plot(2:14, train_score[2:14], type="b", col="green", xlab="# of leaves", ylab="deviance", ylim=c(250, 600))
points(2:14, validation_score[2:14], type="b", col="blue")
legend("topright", legend=c("(green) Train Score", "(blue) Validation Score"), col=c("green","blue"))
# Optimal leaves = 4, depth=4, variables=savings, duration, history
pruned_tree = prune.tree(fit_dt_dev, best=4)
plot(pruned_tree)
text(pruned_tree, pretty=0)
# Predictions for pruned tree on TEST data
fitted_pruned_tree_test = predict(pruned_tree, newdata=test, type="class")
# Confusion matrix for pruned tree on the TEST data
cm_dev_test_prun = table(fitted_pruned_tree_test, test[,20])
# Missclassification rate for pruned tree on the TEST data
mcr_dev_test_prun = 1-sum(diag(cm_dev_test_prun)/sum(cm_dev_test_prun)) #0.248
plot(2:14, train_score[2:14], type="b", col="green", xlab="# of leaves", ylab="deviance", ylim=c(250, 600))
points(2:14, validation_score[2:14], type="b", col="blue")
legend("topright", legend=c("(green) Train Score", "(blue) Validation Score"), col=c("green","blue"))
library(tree)
library(e1071)
library(MASS)
# Task 1
dataframe = read.csv("creditscoring.csv", dec=',')
n = dim(dataframe)[1]
set.seed(12345)
ids = sample(1:n, n) # sample random order of data
# Split the data into training/validation/training (50/25/25)
training = dataframe[ids[1:floor(n/2)],] # 50% of the data
validation = dataframe[ids[(floor(n/2)+1):floor(3*n/4)],] # 25% of data
test = dataframe[ids[(floor(3*n/4)+1):n],] # 25% of the data
# Task 2
fit_dt_dev = tree(good_bad~., data=training, split="deviance")
fit_dt_gini = tree(good_bad~., data=training, split="gini")
# plot(fit_dt_dev)
# text(fit_dt_dev, pretty=0)
# plot(fit_dt_gini)
# text(fit_dt_gini, pretty=0)
# Fit Test Data
fitted_test_dev = predict(fit_dt_dev, test, type="class")
fitted_test_gini = predict(fit_dt_gini, test, type="class")
# Confusion matrixes
cm_dev_test= table(fitted_test_dev, test[,20])
cm_gini_test = table(fitted_test_gini, test[,20])
# Missclassification rates
mcr_dev_test = 1-sum(diag(cm_dev_test))/sum(cm_dev_test)
mcr_gini_test = 1-sum(diag(cm_gini_test))/sum(cm_gini_test)
#mcr_dev_training = 0.2105 = 104 / 494
#mcr_gini_training = 0.2368 = 117 / 494
# mcr_dev < mcr_gini --> Will use Deviance as the measure of impurity
# Task 3
# Empty vectors of scores
train_score = rep(0,14)
validation_score = rep(0,14)
# Compute the deviance for the pruned tree's prediction on train set and on validation set, save in score vectors
for(i in 2:14){
pruned_tree = prune.tree(fit_dt_dev, best=i)
prediction = predict(pruned_tree, newdata=validation, type="tree")
train_score[i] = deviance(pruned_tree)
validation_score[i] = deviance(prediction)
}
# Plot the deviances for each score
plot(2:14, train_score[2:14], type="b", col="green", xlab="# of leaves", ylab="deviance", ylim=c(250, 600))
points(2:14, validation_score[2:14], type="b", col="blue")
legend("topright", legend=c("(green) Train Score", "(blue) Validation Score"), col=c("green","blue"))
# Optimal leaves = 4, depth=4, variables=savings, duration, history
pruned_tree = prune.tree(fit_dt_dev, best=4)
plot(pruned_tree)
text(pruned_tree, pretty=0)
# Predictions for pruned tree on TEST data
fitted_pruned_tree_test = predict(pruned_tree, newdata=test, type="class")
# Confusion matrix for pruned tree on the TEST data
cm_dev_test_prun = table(fitted_pruned_tree_test, test[,20])
# Missclassification rate for pruned tree on the TEST data
mcr_dev_test_prun = 1-sum(diag(cm_dev_test_prun)/sum(cm_dev_test_prun)) #0.248
# Task 4 - Naive Bayes
fit_naive_bayes =naiveBayes(good_bad~., data=training)
fitted_training_naive_bayes = predict(fit_naive_bayes, newdata=training, type="class")
fitted_test_naive_bayes = predict(fit_naive_bayes, newdata=test, type="class")
# Confusion matrixes
cm_naive_bayes_training = table(fitted_training_naive_bayes, training[,20])
cm_naive_bayes_test = table(fitted_test_naive_bayes, test[,20])
# Missclassification rates
mcr_naive_bayes_training = 1-sum(diag(cm_naive_bayes_training))/sum(cm_naive_bayes_training) # 0.3
mcr_naive_bayes_test = 1-sum(diag(cm_naive_bayes_test))/sum(cm_naive_bayes_test) # 0.3
# Task 5 - Loss Matrix
fitted_training_naive_bayes_prob = predict(fit_naive_bayes, newdata=training, type="raw")
fitted_test_naive_bayes_prob = predict(fit_naive_bayes, newdata=test, type="raw")
# Confusion matrixes
cm_naive_bayes_training_loss_matrix = table(fitted_training_naive_bayes_prob[,1]>10/11, training[,20])
cm_naive_bayes_test_loss_matrix = table(fitted_test_naive_bayes_prob[,1]>10/11, test[,20])
pruned_tree = prune.tree(fit_dt_dev, best=4)
plot(pruned_tree)
text(pruned_tree, pretty=0)
depth(pruned_tree)
pruned_tree
summary(pruned_tree)
pruned_tree$where
pruned_tree$terms
pruned_tree$y
pruned_tree$weights
library(tree)
data = read.csv2("State.csv")
data.sorted = data[order(data["MET"]),]
plot(data.sorted$MET, data.sorted$EX, ylim=c(0,450), xlim=c(0,100)) # Some sort of regressional model, 2nd polynomial
nobs = dim(data.sorted)[1]
tree.control = tree.control(nobs, minsize=8)
fit.tree.regression = tree(EX~MET, data=data.sorted, control=tree.control)
set.seed(12354)
cv.tree.regression = cv.tree(fit.tree.regression)
plot(cv.tree.regression$size, cv.tree.regression$dev, type="b", col="forestgreen")
plot(log(cv.tree.regression$k), cv.tree.regression$dev, type="b", col="forestgreen")
plot(cv.tree.regression$size, cv.tree.regression$dev, type="b", col="forestgreen")
plot(cv.tree.regression$size, cv.tree.regression$dev, type="b", col="forestgreen")
plot(log(cv.tree.regression$k), cv.tree.regression$dev, type="b", col="forestgreen")
fit.tree.regression = tree(EX~MET, data=data.sorted, control=tree.control)
fit.tree.regression
predict.tree
predict
fitted = predict(fit.tree.regression, newdata=data.sorted)
plot(fitted)
fitted
plot(data.sorted$MET, fitted.values[2])
plot(data.sorted$MET, fitted.values)
plot(data.sorted$MET, fitted)
points(data.sorted$MET, data.sorted$EX)
plot(data.sorted$MET, fitted, col="green")
points(data.sorted$MET, data.sorted$EX, col="red")
plot(data.sorted$MET, fitted, col="blue")
points(data.sorted$MET, data.sorted$EX, col="red")
fitted
barplot(fitted[2])
barplot(fitted)
fit.tree.regression
fit.tree.regression$weights
resit.tree.regression
hist(data.sorted$MET-fitted)
data.sorted$MET
fitted
hist(data.sorted$Ex-fitted)
hist(data.sorted$EX-fitted)
hist(data.sorted$EX-fitted, main="Residuals")
hist(data.sorted$EX-fitted, main="Residuals", xlab="Residual value", col="forestgreen")
fitted = predict(fit.tree.regression, newdata=data.sorted)
plot(data.sorted$MET, fitted, col="blue")
points(data.sorted$MET, data.sorted$EX, col="red")
residuals = data.sorted$EX-fitted
hist(residuals, main="Residuals", xlab="Residual value", col="forestgreen")
hist(residuals, main="Residuals", xlab="Residual value", col="forestgreen", xlim=c(-150,150))
hist(residuals, main="Residuals", xlab="Residual value", col="forestgreen", xlim=c(-125,125))
fit
fitted
fit.tree.regression
fit.tree.regression$where
fit.tree.regression$terms
fit.tree.regression$call
fit.tree.regression$y
plot(data.sorted$MET, fit.tree.regression$y, col="blue")
points(data.sorted$MET, data.sorted$EX, col="red")
plot(data.sorted$MET, fit.tree.regression$y, col="blue")
points(data.sorted$MET, data.sorted$EX, col="red")
help(tree)
fitted = predict(cv.tree.regression, newdata=data.sorted)
cv.tree.regression
help(tree.control)

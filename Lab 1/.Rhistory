}
ROC
install.packages("kknn")
install.packages("kknn")
library(kknn)
setwd("~/TDDE01/Machine-Learning-Labs/Lab 1")
library(kknn)
knearest = function(data, k, newdata) {
knearest = function(data, k, newdata) {
n1=dim(data)[1] #number of observations in training set
n2=dim(newdata)[1] #number of observations in test set
p=dim(data)[2] #number of variables observed in both sets (including whether it is spam or not)
Prob=numeric(n2) #vector as long as test data, to be filled with probabilities for spam or not spam
X=as.matrix(data[,-p]) # matrix of the training data excluding the spam/not spam
Y=as.matrix(newdata[,-p]) # matrix is the test data excludign the spam/not spam
# i
X=X/matrix(sqrt(rowSums(X^2)), nrow=n1, ncol=p-1) #divide each Xi on row j by sqrt of the sum of the square of all Xij from current row j
# ii
Y=Y/matrix(sqrt(rowSums(Y^2)), nrow=n2, ncol=p-1) #same but for Y
# iii
C=X%*%t(Y)
D=1-C
print(D)
for (i in 1:n2 ){
nearest_neighbors_indexes = order(D[,i])[1:k] # indexes of the 5 nearest
Prob[i] = sum(data[nearest_neighbors_indexes,p])/k # the probability of it being spam or not Ki/K
}
return(Prob)
}
ROC = function(Y, Yfit, p){
m=length(p)
TPR=numeric(m)
FPR=numeric(m)
for(i in 1:m){
Ypred=ifelse(Yfit>p[i],1,0)
t=table('pred'=Ypred, 'true'=Y)
print(t)
TPR[i]=t[1,1]/sum(t[,1])
FPR[i]=t[1,2]/sum(t[,2])
}
return (list(TPR=TPR,FPR=FPR))
}
dataframe = read.csv("spambase.csv", dec=',')
n = dim(dataframe)[1]
set.seed(12345)
id=sample(1:n, floor(n/2))
train = dataframe[id,]
test = dataframe[-id,]
# Prob
Prob_k5 = knearest(train, 5, test)
Prob_k1 = knearest(train, 1, test)
Prob_k5_kknn = kknn(Spam ~ .,train=train, test=test, k=5)
#confusion matrixes
cm_k5 = table(Prob_k5>0.5, Prob_k5)
cm_k1 = table(Prob_k1>0.5, Prob_k1)
cm_k5_kknn = table(Prob_k5_kknn>0.5, Prob_k5_kknn)
#missclassification rate
mcr_k5 = 1-sum(diag(cm_k5)/sum(cm_k5))
mcr_k1 = 1-sum(diag(cm_k1)/sum(cm_k1))
mcr_k5_kknn = 1-sum(diag(cm_k5_kknn)/sum(cm_k5_kknn))
# ROC and sensitivty
p_seq = seq(from=0.05, to=0.95, by=0.05)
#debugonce(ROC)
list_result = ROC(test[,49], Prob_k5, p_seq)
plot(x=list_result$FPR, y=list_result$TPR, xlab="FPR", ylab="TPR", xlim=c(0,1), ylim=c(0,1))
knearest = function(data, k, newdata) {
n1=dim(data)[1] #number of observations in training set
n2=dim(newdata)[1] #number of observations in test set
p=dim(data)[2] #number of variables observed in both sets (including whether it is spam or not)
Prob=numeric(n2) #vector as long as test data, to be filled with probabilities for spam or not spam
X=as.matrix(data[,-p]) # matrix of the training data excluding the spam/not spam
Y=as.matrix(newdata[,-p]) # matrix is the test data excludign the spam/not spam
# i
X=X/matrix(sqrt(rowSums(X^2)), nrow=n1, ncol=p-1) #divide each Xi on row j by sqrt of the sum of the square of all Xij from current row j
# ii
Y=Y/matrix(sqrt(rowSums(Y^2)), nrow=n2, ncol=p-1) #same but for Y
# iii
C=X%*%t(Y)
D=1-C
print(D)
for (i in 1:n2 ){
nearest_neighbors_indexes = order(D[,i])[1:k] # indexes of the 5 nearest
Prob[i] = sum(data[nearest_neighbors_indexes,p])/k # the probability of it being spam or not Ki/K
}
return(Prob)
}
ROC = function(Y, Yfit, p){
m=length(p)
TPR=numeric(m)
FPR=numeric(m)
for(i in 1:m){
Ypred=ifelse(Yfit>p[i],1,0)
t=table('pred'=Ypred, 'true'=Y)
print(t)
TPR[i]=t[1,1]/sum(t[,1])
FPR[i]=t[1,2]/sum(t[,2])
}
return (list(TPR=TPR,FPR=FPR))
}
dataframe = read.csv("spambase.csv", dec=',')
n = dim(dataframe)[1]
set.seed(12345)
id=sample(1:n, floor(n/2))
train = dataframe[id,]
test = dataframe[-id,]
# Prob
Prob_k5 = knearest(train, 5, test)
Prob_k1 = knearest(train, 1, test)
Prob_k5_kknn = kknn(Spam ~ .,train=train, test=test, k=5)
#confusion matrixes
cm_k5 = table(Prob_k5>0.5, Prob_k5)
cm_k1 = table(Prob_k1>0.5, Prob_k1)
cm_k5_kknn = table(Prob_k5_kknn>0.5, Prob_k5_kknn)
#missclassification rate
cm_k1 = table(Prob_k1>0.5, Prob_k1)
cm_k5_kknn = table(Prob_k5_kknn>0.5, Prob_k5_kknn)
cm_k5_kknn
cm_k5_kknn = table(Prob_k5_kknn>0.5, Prob_k5_kknn)
Prob_k5_kknn = kknn(Spam ~ .,train=train, test=test, k=5)
Prob_k5_kknn
Prob_k5_kknn = kknn(Spam ~ .,train=train, test=test, k=5)
Prob_k5_kknn
Prob_k5_kknn$prob
Prob_k5_kknn$fitted.values
cm_k5_kknn = table(fitted(Prob_k5_kknn)>0.5, fitted(Prob_k5_kknn))
cm_k5_kknn
loglikelihood = function(x, θ){
return(length(x)*log(θ)-θ*sum(x))
}
ax_loglikelihood = function(x){
return(length(x)/sum(x))
}
og_bayesian = function(x, θ, λ){
n = length(x)
#return(n*log(θ)-θ*sum(x)+n*log(λ)-n*λ*θ)
return(n*log(θ)-θ*sum(x)+log(λ)-λ*θ)
}
ax_bayesian = function(x, λ){
n = length(x)
#return (n/(sum(x)+n*λ))
return (n/(sum(x)+λ))
}
dataframe = read.csv("machines.csv", dec=',')
X = dataframe[1]$Length # X vector
X_6 = X[1:6] # X vector of first 6 values
θ = seq(from=0, to=20, by=0.025) # θs to test
m = length(θ) # Amount of θs to be tested
loglikelihood_n = numeric(m) # Empty vector for loglikelihood values with all x values
loglikelihood_6 = numeric(m) # Empty vector for loglikelihood values the first 6 values
# Task 2
# Calculate the loglikelihoods for different θ values with the given vectors
for(i in 1:m){
loglikelihood_n[i] = loglikelihood(X, θ[i])
loglikelihood_6[i] = loglikelihood(X_6, θ[i])
}
# Task 3
# Plot the loglikelihoods
plot(θ, loglikelihood_6, type="l", main="Dependence of Log-Likelihood and θ (n values)", xlab="θ", ylab="Log-Likelihood", col="green")
lines(θ, loglikelihood_n, col="blue")
# θ values for maximum loglikelihoods
θstar_n = max_loglikelihood(X)
θstar_6 = max_loglikelihood(X_6)
# Task 4
# Plot the Bayesian Model
λ = 10;
log_bayesian_model = numeric(m)
for(i in 1:m){
log_bayesian_model[i] = log_bayesian(X, θ[i], λ)
}
plot(θ, log_bayesian_model, type="l", xlim=c(0,20), ylim=c(-500, 0), main="Bayesian model", xlab="θ", ylab="Log-Bayesian", col="green")
θstar_b = max_bayesian(X, λ)
log_bayesian = function(x, θ, λ){
n = length(x)
#return(n*log(θ)-θ*sum(x)+n*log(λ)-n*λ*θ)
return(n*log(θ)-θ*sum(x)+log(λ)-λ*θ)
}
log_bayesian_model = numeric(m)
for(i in 1:m){
for(i in 1:m){
log_bayesian_model[i] = log_bayesian(X, θ[i], λ)
}
plot(θ, log_bayesian_model, type="l", xlim=c(0,20), ylim=c(-500, 0), main="Bayesian model", xlab="θ", ylab="Log-Bayesian", col="green")
plot(θ, log_bayesian_model, type="l", xlim=c(0,20), ylim=c(-500, 0), main="Bayesian model", xlab="θ", ylab="Log-Bayesian", col="green")
log_bayesian = function(x, θ, λ){
n = length(x)
#return(n*log(θ)-θ*sum(x)+n*log(λ)-n*λ*θ)
return(n*log(θ)-θ*sum(x)+log(λ)-λ*θ)
}
λ = 10;
log_bayesian_model = numeric(m)
for(i in 1:m){
log_bayesian_model[i] = log_bayesian(X, θ[i], λ)
}
plot(θ, log_bayesian_model, type="l", xlim=c(0,20), ylim=c(-500, 0), main="Bayesian model", xlab="θ", ylab="Log-Bayesian", col="green")
θstar_b = max_bayesian(X, λ)
max_bayesian = function(x, λ){
n = length(x)
#return (n/(sum(x)+n*λ))
return (n/(sum(x)+λ))
}
θstar_b = max_bayesian(X, λ)
m
θstar_n = max_loglikelihood(X)
log_bayesian(X, 3, 10)
X
log_bayesian(dataframe, 3, 10)
X = dataframe[1]$Length # X vector
log_bayesian(X, 3, 10)
X
Prob_k5_kknn = kknn(Spam ~ .,train=train, test=test, k=5)
cm_k5_kknn = table(fitted(Prob_k5_kknn)>0.5, fitted(Prob_k5_kknn))
cm_k5_kknn
length(X)
Prob_k5_kknn = kknn(Spam ~ .,train=train, test=test, k=5)
Prob_k5_kknn
Prob_k5_kknn$fitted.values
cm_k5_kknn = table(Prob_k5_kknn$fitted.values>0.5, Prob_k5_kknn$fitted.values)
cm_k5_kknn
cm_k5_kknn = table(Prob_k5_kknn$fitted.values>0.5, Prob_k5_kknn$response)
cm_k5_kknn = table(Prob_k5_kknn$response>0.5, Prob_k5_kknn$response)
cm_k5_kknn
cm_k5 = table(Prob_k5>0.5, Prob_k5)
cm_k5
test[,49]
cm_k5 = table(Prob_k5>0.5, test[,49])
cm_k5
cm_k5 = table(Prob_k5>0.5, test[,49])
cm_k1 = table(Prob_k1>0.5, test[,49])
cm_k5_kknn = table(Prob_k5_kknn$fitted.values>0.5, test[,49])
cm_k5
cm_k5_kknn
cm_k5_kknn = table(test[,49], Prob_k5_kknn$fitted.values>0.5)
cm_k5_kknn
cm_k5_kknn = table(Prob_k5_kknn$fitted.values<0.5, test[,49])
cm_k5_kknn
mcr_k5_kknn = 1-sum(diag(cm_k5_kknn)/sum(cm_k5_kknn))
mcr_k1 = 1-sum(diag(cm_k1)/sum(cm_k1))
mcr_k5 = 1-sum(diag(cm_k5)/sum(cm_k5))
mcr_k5
mcr_k5_kknn
cm_k5 = table(Prob_k5>0.5, test[,49])
cm_k1 = table(Prob_k1>0.5, test[,49])
cm_k5_kknn = table(Prob_k5_kknn$fitted.values>0.5, test[,49])
mcr_k5 = 1-sum(diag(cm_k5)/sum(cm_k5))
mcr_k1 = 1-sum(diag(cm_k1)/sum(cm_k1))
mcr_k5_kknn = 1-sum(diag(cm_k5_kknn)/sum(cm_k5_kknn))
mcr_k5
mcr_k5_kknn
hist(new_observations, col="green", xlim=c(0,5), ylim=c(0,25), main="Original observations", xlab="x")
set.seed(12345)
new_observations = rexp(n=50, rate = θstar_n)
θstar_n = max_loglikelihood(X)
max_loglikelihood = function(x){
return(length(x)/sum(x))
}
set.seed(12345)
new_observations = rexp(n=50, rate = θstar_n)
θstar_n = max_loglikelihood(X)
new_observations = rexp(n=50, rate = θstar_n)
hist(new_observations, col="green", xlim=c(0,5), ylim=c(0,25), main="Original observations", xlab="x")
x11()
hist(X, col="blue", xlim=c(0,5), ylim=c(0,25), main="New observations", xlab="x")
# returns the loglikelihood value for given θ and vector X
loglikelihood = function(x, θ){
return(length(x)*log(θ)-θ*sum(x))
}
max_loglikelihood = function(x){
return(length(x)/sum(x))
}
log_bayesian = function(x, θ, λ){
n = length(x)
#return(n*log(θ)-θ*sum(x)+n*log(λ)-n*λ*θ)
return(n*log(θ)-θ*sum(x)+log(λ)-λ*θ)
}
max_bayesian = function(x, λ){
n = length(x)
#return (n/(sum(x)+n*λ))
return (n/(sum(x)+λ))
}
dataframe = read.csv("machines.csv", dec=',')
X = dataframe[1]$Length # X vector
X_6 = X[1:6] # X vector of first 6 values
θ = seq(from=0, to=20, by=0.025) # θs to test
m = length(θ) # Amount of θs to be tested
loglikelihood_n = numeric(m) # Empty vector for loglikelihood values with all x values
loglikelihood_6 = numeric(m) # Empty vector for loglikelihood values the first 6 values
for(i in 1:m){
loglikelihood_n[i] = loglikelihood(X, θ[i])
loglikelihood_6[i] = loglikelihood(X_6, θ[i])
}
plot(θ, loglikelihood_6, type="l", main="Dependence of Log-Likelihood and θ (n values)", xlab="θ", ylab="Log-Likelihood", col="green")
lines(θ, loglikelihood_n, col="blue")
# θ values for maximum loglikelihoods
θstar_n = max_loglikelihood(X)
θstar_6 = max_loglikelihood(X_6)
# Task 4
# Plot the Bayesian Model
λ = 10;
log_bayesian_model = numeric(m)
for(i in 1:m){
log_bayesian_model[i] = log_bayesian(X, θ[i], λ)
}
plot(θ, log_bayesian_model, type="l", xlim=c(0,20), ylim=c(-500, 0), main="Bayesian model", xlab="θ", ylab="Log-Bayesian", col="green")
θstar_b = max_bayesian(X, λ)
# Task 5
# Compare new generated observations to original ones
set.seed(12345)
new_observations = rexp(n=50, rate = θstar_n)
#data.frame(X, new_observations)
hist(new_observations, col="green", xlim=c(0,5), ylim=c(0,25), main="Original observations", xlab="x")
x11()
hist(X, col="blue", xlim=c(0,5), ylim=c(0,25), main="New observations", xlab="x")
hist(new_observations, col="green", xlim=c(0,5), ylim=c(0,25), main="Original observations", xlab="x")
hist(new_observations, col="green", xlim=c(0,5), ylim=c(0,30), main="Original observations", xlab="x")
set.seed(12345)
new_observations = rexp(n=50, rate = θstar_n)
#data.frame(X, new_observations)
hist(X, col="green", xlim=c(0,5), ylim=c(0,30), main="Original observations", xlab="x")
x11()
hist(new_observations, col="blue", xlim=c(0,5), ylim=c(0,25), main="New observations", xlab="x")
# returns the loglikelihood value for given θ and vector X
loglikelihood = function(x, θ){
return(length(x)*log(θ)-θ*sum(x))
}
# returns the max logilikelihood value for the given distribution estimation
max_loglikelihood = function(x){
return(length(x)/sum(x))
}
# returns log value of the bayesian proportional probability for given θ, λ and vector X
log_bayesian = function(x, θ, λ){
n = length(x)
#return(n*log(θ)-θ*sum(x)+n*log(λ)-n*λ*θ)
return(n*log(θ)-θ*sum(x)+log(λ)-λ*θ)
}
max_bayesian = function(x, λ){
n = length(x)
#return (n/(sum(x)+n*λ))
return (n/(sum(x)+λ))
}
###########################################################################
# Task 1
# Data preparation
dataframe = read.csv("machines.csv", dec=',')
X = dataframe[1]$Length # X vector
X_6 = X[1:6] # X vector of first 6 values
θ = seq(from=0, to=20, by=0.025) # θs to test
m = length(θ) # Amount of θs to be tested
loglikelihood_n = numeric(m) # Empty vector for loglikelihood values with all x values
loglikelihood_6 = numeric(m) # Empty vector for loglikelihood values the first 6 values
# Task 2
# Calculate the loglikelihoods for different θ values with the given vectors
for(i in 1:m){
loglikelihood_n[i] = loglikelihood(X, θ[i])
loglikelihood_6[i] = loglikelihood(X_6, θ[i])
}
# Task 3
# Plot the loglikelihoods
plot(θ, loglikelihood_6, type="l", main="Dependence of Log-Likelihood and θ (n values)", xlab="θ", ylab="Log-Likelihood", col="green")
lines(θ, loglikelihood_n, col="blue")
# θ values for maximum loglikelihoods
θstar_n = max_loglikelihood(X)
θstar_6 = max_loglikelihood(X_6)
# Task 4
# Plot the Bayesian Model
λ = 10;
log_bayesian_model = numeric(m)
for(i in 1:m){
log_bayesian_model[i] = log_bayesian(X, θ[i], λ)
}
plot(θ, log_bayesian_model, type="l", xlim=c(0,20), ylim=c(-500, 0), main="Bayesian model", xlab="θ", ylab="Log-Bayesian", col="green")
θstar_b = max_bayesian(X, λ)
set.seed(12345)
new_observations = rexp(n=50, rate = θstar_n)
#data.frame(X, new_observations)
hist(X, col="green", xlim=c(0,5), ylim=c(0,30), main="Original observations", xlab="x")
x11()
hist(new_observations, col="blue", xlim=c(0,5), ylim=c(0,30), main="New observations", xlab="x")
hist(X, col="green", xlim=c(0,5), ylim=c(0,30), main="Original observations", xlab="x")
x11()
hist(new_observations, col="blue", xlim=c(0,5), ylim=c(0,30), main="New observations", xlab="x")
et.seed(12345)
set.seed(12345)
new_observations = rexp(n=50, rate = θstar_n)
hist(new_observations, col="blue", xlim=c(0,10), ylim=c(0,30), main="New observations", xlab="x")
hist(X, col="green", xlim=c(0,8), ylim=c(0,30), main="Original observations", xlab="x")
x11()
hist(new_observations, col="blue", xlim=c(0,8), ylim=c(0,30), main="New observations", xlab="x")
new_observations
X
help(hist)
hist(new_observations, col="blue", breaks=0.5, xlim=c(0,8), ylim=c(0,30), main="New observations", xlab="x")
hist(new_observations, col="blue", breaks=c(0,0.5), xlim=c(0,8), ylim=c(0,30), main="New observations", xlab="x")
hist(new_observations, col="blue", breaks=20, xlim=c(0,8), ylim=c(0,30), main="New observations", xlab="x")
hist(new_observations, col="blue", breaks=20, xlim=c(0,8), ylim=c(0,30), main="New observations", xlab="x")
#data.frame(X, new_observations)
hist(X, col="green", xlim=c(0,8), ylim=c(0,30), main="Original observations", xlab="x")
hist(X, col="green", xlim=c(0,5), ylim=c(0,30), main="Original observations", xlab="x")
hist(new_observations, col="blue", breaks=10, xlim=c(0,5), ylim=c(0,30), main="New observations", xlab="x")
hist(X, col="green", breaks=16, xlim=c(0,8), ylim=c(0,30), main="Original observations", xlab="x")
x11()
hist(new_observations, col="blue", breaks=16, xlim=c(0,8), ylim=c(0,30), main="New observations", xlab="x")
hist(X, col="green", xlim=c(0,8), ylim=c(0,30), main="Original observations", xlab="x")
x11()
hist(new_observations, col="blue", breaks=16, xlim=c(0,8), ylim=c(0,30), main="New observations", xlab="x")
hist(X, col="green", xlim=c(0,6), ylim=c(0,30), main="Original observations", xlab="x")
x11()
hist(new_observations, col="blue", breaks=12, xlim=c(0,6), ylim=c(0,30), main="New observations", xlab="x")
dataframe = read.csv("tecator.csv", dec=',')[-215]
dataframe
tecator.csv
dataframe = read.csv("tecator.csv", dec=',')
dataframe = dataframe[-215,]
plot(model0_ridge, xvar="lambda")
library(MASS)
library(glmnet)
# Task 1
dataframe = read.csv("tecator.csv", dec=',')
dataframe = dataframe[-215,]
protein = dataframe$Protein
moisture = dataframe$Moisture
fat = dataframe$Fat
channels = as.matrix(dataframe[1:100])
plot(protein, moisture)
# Task 2
# M1 = w0+w1*p+ε, where ε = N~(0, σ)
# M2 = w0+w1*p+w2*p²+ε, where ε=N~(0, σ)
# M3 = w0+w1*p+w2*p²+w3*p³+ε, where ε=N~(0, σ)
# etc.
# MSE criterion is appropriate to use since it minimizes the errors in our predicions (deviation of ε)
# Task 3
n = dim(dataframe)[1]
ids = sample(1:n, floor(n/2))
training = dataframe[ids,]
validation = dataframe[-ids,]
p = training$Protein
p2 = p^2
p3 = p^3
p4 = p^4
p5 = p^5
p6 = p^6
X1 = cbind(p)
X2 = cbind(p, p2)
X3 = cbind(p, p2, p3)
X4 = cbind(p, p2, p3, p4)
X5 = cbind(p, p2, p3, p4, p5)
X6 = cbind(p, p2, p3, p4, p5, p6)
#B = solve(t(X)%*%X)%*%t(X)%*%Y if I was to calculate it myself, did not work for higher polynomial terms of power
M1 = lm(Moisture ~ X1, data=training)
M2 = lm(Moisture ~ X2, data=training)
M3 = lm(Moisture ~ X3, data=training)
M4 = lm(Moisture ~ X4, data=training)
M5 = lm(Moisture ~ X5, data=training)
M6 = lm(Moisture ~ X6, data=training)
# Make the predictions
fitted_validation1 = predict(M1, validation)
fitted_validation2 = predict(M2, validation)
fitted_validation3 = predict(M3, validation)
fitted_validation4 = predict(M4, validation)
fitted_validation5 = predict(M5, validation)
fitted_validation6 = predict(M6, validation)
fitted_training1 = predict(M1)
fitted_training2 = predict(M2)
fitted_training3 = predict(M3)
fitted_training4 = predict(M4)
fitted_training5 = predict(M5)
fitted_training6 = predict(M6)
# Calculate the MSEs
# Validation MSEs
mse_v1 = mean((validation$Moisture-fitted_validation1)^2)
mse_v2 = mean((validation$Moisture-fitted_validation2)^2)
mse_v3 = mean((validation$Moisture-fitted_validation3)^2)
mse_v4 = mean((validation$Moisture-fitted_validation4)^2)
mse_v5 = mean((validation$Moisture-fitted_validation5)^2)
mse_v6 = mean((validation$Moisture-fitted_validation6)^2)
# Training MSEs
mse_t1 = mean((training$Moisture-fitted_training1)^2)
mse_t2 = mean((training$Moisture-fitted_training2)^2)
mse_t3 = mean((training$Moisture-fitted_training3)^2)
mse_t4 = mean((training$Moisture-fitted_training4)^2)
mse_t5 = mean((training$Moisture-fitted_training5)^2)
mse_t6 = mean((training$Moisture-fitted_training6)^2)
# MSEs for validation and training
mse_v = c(mse_v1, mse_v2, mse_v3, mse_v4, mse_v5, mse_v6)
mse_t = c(mse_t1, mse_t2, mse_t3, mse_t4, mse_t5, mse_t6)
idx = !colnames(training) %in% c("Sample", "Fat", "Protein", "Moisture")
formulax = paste("Fat~", paste(colnames(training[,idx]), collapse = "+"))
fit = lm(formula(formulax), data=dataframe)
step = stepAIC(fit, direction="both", trace=FALSE)
step$anova
summary(step)
scaled_channels = scale(dataframe[,2:101])
scaled_fat = scale(dataframe$Fat)
model0_ridge = glmnet(as.matrix(scaled_channels), scaled_fat, alpha=0, family="gaussian")
plot(model0_ridge, xvar="lambda")
library(glmnet)
install.packages("glmnet")
library(glmnet)
model0_ridge = glmnet(as.matrix(scaled_channels), scaled_fat, alpha=0, family="gaussian")
plot(model0_ridge, xvar="lambda")
plot(model0_ridge, xvar="lambda")
